{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIT2020116_ML Assignment 2_Q1(part-I).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDFZ8cQ80Jyq"
      },
      "source": [
        "#Q-1. Create an ANN with one hidden layer and do classification on\n",
        "#the datasets given in the link:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "cY_lToASPm8d",
        "outputId": "e1f52273-8c3d-4f0b-99b9-fb99018bfbcf"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "#Check observation one by one different dataset and see observations.\n",
        "\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/psantul/Dataset/main/data.csv\", encoding='latin-1')\n",
        "#data = pd.read_csv(\"https://raw.githubusercontent.com/psantul/Dataset/main/titanic.csv\",delimiter = ',', encoding='latin-1')\n",
        "print(data.shape)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(569, 33)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0    842302         M        17.99         10.38          122.80     1001.0   \n",
              "1    842517         M        20.57         17.77          132.90     1326.0   \n",
              "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
              "3  84348301         M        11.42         20.38           77.58      386.1   \n",
              "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
              "\n",
              "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0          0.11840           0.27760          0.3001              0.14710   \n",
              "1          0.08474           0.07864          0.0869              0.07017   \n",
              "2          0.10960           0.15990          0.1974              0.12790   \n",
              "3          0.14250           0.28390          0.2414              0.10520   \n",
              "4          0.10030           0.13280          0.1980              0.10430   \n",
              "\n",
              "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
              "0  ...          17.33           184.60      2019.0            0.1622   \n",
              "1  ...          23.41           158.80      1956.0            0.1238   \n",
              "2  ...          25.53           152.50      1709.0            0.1444   \n",
              "3  ...          26.50            98.87       567.7            0.2098   \n",
              "4  ...          16.67           152.20      1575.0            0.1374   \n",
              "\n",
              "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
              "0             0.6656           0.7119                0.2654          0.4601   \n",
              "1             0.1866           0.2416                0.1860          0.2750   \n",
              "2             0.4245           0.4504                0.2430          0.3613   \n",
              "3             0.8663           0.6869                0.2575          0.6638   \n",
              "4             0.2050           0.4000                0.1625          0.2364   \n",
              "\n",
              "   fractal_dimension_worst  Unnamed: 32  \n",
              "0                  0.11890          NaN  \n",
              "1                  0.08902          NaN  \n",
              "2                  0.08758          NaN  \n",
              "3                  0.17300          NaN  \n",
              "4                  0.07678          NaN  \n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6JC1Mz8ad3C"
      },
      "source": [
        "x = data.iloc[:,3:].values\n",
        "y = data.iloc[:,1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hfncJqhan-w"
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtGGD6MGbCvJ"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSQCSAE5bTvz",
        "outputId": "35082a46-29da-4d50-f75b-8a44ab3552e5"
      },
      "source": [
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E:\\Users\\Arun Kumar\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:770: RuntimeWarning: invalid value encountered in true_divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "E:\\Users\\Arun Kumar\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:711: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
            "  result = op(x, *args, **kwargs)\n",
            "E:\\Users\\Arun Kumar\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:770: RuntimeWarning: invalid value encountered in true_divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "E:\\Users\\Arun Kumar\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py:711: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
            "  result = op(x, *args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s08J_0DbsYL"
      },
      "source": [
        "class NeuralNetwork:\n",
        "    \n",
        "    def __init__(self,X, y, X_test, y_test, hidden_nodes=1, learning_rate=0.1, epochs=5000):\n",
        "        \n",
        "        #data\n",
        "        self.y = y[:,None]\n",
        "        self.X = X\n",
        "        \n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "\n",
        "        #parameters\n",
        "        np.random.seed(4)\n",
        "        self.input_nodes = len(X[0])\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = self.y.shape[1]\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        #init weights\n",
        "        self.w1 = 2*np.random.random((self.input_nodes, self.hidden_nodes)) - 1\n",
        "        self.w2 = 2*np.random.random((self.hidden_nodes, self.output_nodes)) - 1\n",
        "\n",
        "        self.train(epochs)\n",
        "        self.test()\n",
        "        \n",
        "    def sigmoid(self,X):\n",
        "        return (1/(1+np.exp(-X)))\n",
        "\n",
        "    def sigmoid_prime(self,X):\n",
        "        return X * (1 - X)\n",
        "        \n",
        "    def train(self, epochs):\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            \n",
        "            #FORWARD PROPAGATION\n",
        "            \n",
        "            #hidden layer\n",
        "            # W1(398,30) X(30,12)\n",
        "            l1 = self.sigmoid(np.dot(self.X, self.w1))\n",
        "\n",
        "            #output layer\n",
        "            #l1(398,12) W2(12,1)\n",
        "            l2 = self.sigmoid(np.dot(l1, self.w2))\n",
        "        \n",
        "            # BACKPROPAGATION\n",
        "            \n",
        "            #calculate how far off our prediciton was\n",
        "            error = self.y-l2\n",
        "            \n",
        "            #calculate how far off each layer is\n",
        "            l2_delta = error * self.sigmoid_prime(l2)\n",
        "            l1_delta = l2_delta.dot(self.w2.T) * self.sigmoid_prime(l1)\n",
        "\n",
        "            #update weights with our newly found error values\n",
        "            self.w2 = np.add(self.w2, l1.T.dot(l2_delta) * self.learning_rate)\n",
        "            self.w1 = np.add(self.w1, self.X.T.dot(l1_delta) * self.learning_rate)\n",
        "        \n",
        "        #print('Error:', (abs(error)).mean())\n",
        "    \n",
        "    def test(self):\n",
        "        correct = 0\n",
        "        pred_list = []\n",
        "        \n",
        "        #replicate feedforward network for testing\n",
        "        l1 = self.sigmoid(np.dot(self.X_test, self.w1))\n",
        "        l2 = self.sigmoid(np.dot(l1, self.w2))\n",
        "        \n",
        "        #loop through all of the outputs of layer 2\n",
        "        for i in range(len(l2)):\n",
        "            if l2[i] >= 0.5:\n",
        "                pred = 1\n",
        "            else:\n",
        "                pred = 0\n",
        "\n",
        "            if pred == self.y_test[i]:\n",
        "                correct += 1\n",
        "                \n",
        "            pred_list.append(pred)\n",
        "        print(pred_list)\n",
        "        print(y_test)\n",
        "        correct = np.sum(pred_list == y_test)\n",
        "        print(\"%d out of %d predictions correct\" % (correct, len(pred_list)))\n",
        "\n",
        "        print(\"Test Accuracy: \", ((correct/len(y_test))*100),'%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhHcVoWIeWTd",
        "outputId": "e3640638-e707-4eab-c641-8c97694c485a"
      },
      "source": [
        "nn = NeuralNetwork(X_train, y_train, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0\n",
            " 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
            " 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0\n",
            " 1 1 0]\n",
            "67 out of 114 predictions correct\n",
            "Test Accuracy:  58.77192982456141 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}